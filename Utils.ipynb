{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706c6ebd",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Summary Statistics\n",
    "- Data Cleaning\n",
    "- Dataframe Operations\n",
    "- Preprocessing\n",
    "- EDA\n",
    "  - Univariate Analysis\n",
    "  - Bivariate Analysis\n",
    "- Library Imports    \n",
    "- Ml Algos\n",
    "    - Regression\n",
    "    - Classification\n",
    "    - Clustering\n",
    "- Pipeline  \n",
    "- Evaluation Metrics\n",
    "- Neural Networks\n",
    "    - Regression\n",
    "    - Classification\n",
    "- *GPU Support*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68478ab",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "- df.head()\n",
    "- df.info()\n",
    "- Numerical column summary\n",
    "```\n",
    "df.describe().transpose()\n",
    "```\n",
    "- Categorical column summary\n",
    "```\n",
    "df.describe(include=['O']).transpose()\n",
    "```\n",
    "- df.shape\n",
    "- Get columns with missing values\n",
    "```\n",
    "missing_value_df = round(df.isnull().sum() / len(df)  * 100,2)\n",
    "missing_value_df[missing_value_df > 0].sort_values(ascending=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e322ba3",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f255a2b",
   "metadata": {},
   "source": [
    "#### 1. Duplicate rows\n",
    "df[df.duplicated()]\n",
    "\n",
    "#### 2. Drop columns where all values are same\n",
    "df.columns[df.nunique() <= 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5461c6",
   "metadata": {},
   "source": [
    "### Dataframe Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83667f8f",
   "metadata": {},
   "source": [
    "####  1. Drop columns\n",
    "df.drop(columns=column_list)\n",
    "\n",
    "####  2. Get columns whose name starting with\n",
    "```\n",
    "fb_user_cols = [col for col in df.columns if 'fb_user' in col]\n",
    "df[fb_user_cols]\n",
    "```\n",
    "####  3. Categorical columns\n",
    "```\n",
    "# get categorical columns\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "```\n",
    "####  4. Numerical columns\n",
    "```\n",
    "# get numerical columns\n",
    "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "```\n",
    "#### 5. Filter column\n",
    "```\n",
    "df[['count_rech_3g_6', 'arpu_3g_6', 'monthly_3g_6', 'sachet_3g_6']][df.arpu_3g_6.isnull()][:5]\n",
    "```\n",
    "#### 6. Row wise sum \n",
    "```\n",
    "df['churn'] = df[['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']].sum(axis = 1) == 0\n",
    "```\n",
    "#### 7. Column wise sum \n",
    "```\n",
    "df['churn'] = df[['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']].sum(axis = 0) == 0\n",
    "```\n",
    "#### 8. Value Counts %\n",
    "```\n",
    "(df.churn.value_counts() / len(df) * 100).sort_values(ascending=False)\n",
    "```\n",
    "#### 9. Dataframe get columns whose name contains the string\n",
    "```\n",
    "def get_col(df, col_str):\n",
    "    '''\n",
    "    returns column names of df having the stringcol_str\n",
    "    '''\n",
    "    return np.array([col for col in df.columns if col_str in col])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc61bd",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Missing Value Percentage\n",
    "```\n",
    "missing_value_df = round(df.isnull().sum() / len(df)  * 100,2)\n",
    "missing_value_df[missing_value_df > 0].sort_values(ascending=False)\n",
    "```\n",
    "- Missing Value Imputation\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer(strategy='median')\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "```\n",
    "- Fill null with 0\n",
    "```\n",
    "df[rech_cols] = df[rech_cols].apply(lambda x: x.fillna(0))\n",
    "```\n",
    "- Categorical columns\n",
    "\n",
    "    **ordinal encoding**\n",
    "```\n",
    "def label_encode(val, mapping):\n",
    "    return mapping[val]\n",
    "```\n",
    "```\n",
    "mapping_utilities = {'ELO' : 1, 'NoSeWa' : 2, 'NoSewr': 3, 'AllPub' : 4}\n",
    "df['Utilities'] = df['Utilities'].apply(lambda val :        label_encode(val,mapping_utilities))\n",
    "```\n",
    "\n",
    "    *Same as above usking sklearn*\n",
    "```\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "label_X_train[good_label_cols] = enc.fit_transform(label_X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = enc.transform(label_X_valid[good_label_cols])\n",
    "```\n",
    "\n",
    "    *In the case that the validation data contains values that don't appear in the training data, the encoder will throw an error*\n",
    "\n",
    "```\n",
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "```\n",
    "\n",
    "   **nominal encoding**\n",
    "    \n",
    "```\n",
    "cat_cols = [\"MSZoning\",\"Street\",\"Alley\"]\n",
    "df_cat = df[cat_cols]\n",
    "df_cat_dummies = pd.get_dummies(df_cat, drop_first=True)\n",
    "df = df.drop(cat_cols, axis=1)\n",
    "df = pd.concat([df, df_cat_dummies], axis=1)\n",
    "```\n",
    "\n",
    "*The above is going to have probolems if there are some categories present in train but not in test, so use below*\n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Use as many lines of code as you need!\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_X_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "OH_X_train.index = X_train.index\n",
    "OH_X_valid.index = X_valid.index\n",
    "\n",
    "num_X_train = X_train.drop(low_cardinality_cols + high_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(low_cardinality_cols + high_cardinality_cols, axis=1)\n",
    "\n",
    "OH_X_train = pd.concat([num_X_train, OH_X_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_X_valid], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac72528",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "#### Univariate analysis\n",
    "\n",
    "- Plots - distplot for numerical & countplot for categorical\n",
    "```\n",
    "def uni(col):\n",
    "    '''\n",
    "        distplot for numerical\n",
    "        countplot for categorical\n",
    "    '''\n",
    "    if 'int' in str(col.dtype):\n",
    "        sns.distplot(col)\n",
    "    elif 'int' in str(col.dtype):\n",
    "        sns.countplot(col)\n",
    "```\n",
    "- Sub plots\n",
    "```\n",
    "plt.figure(figsize=(16, 12))\n",
    "uni_cols = list(get_col(high_value_cust_df, 'net'))\n",
    "for col in uni_cols:\n",
    "    plt.subplot(3,3, uni_cols.index(col) + 1)\n",
    "    uni(high_value_cust_df[col])\n",
    "```\n",
    "\n",
    "#### Bivariate analysis\n",
    "- Categorical vs numerical relationship\n",
    "```\n",
    "sns.boxplot(data=capped_df, x='churn', y='aon')\n",
    "```\n",
    "- Correlations\n",
    "```\n",
    "df.corr()\n",
    "sns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edeabc8",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "- Pandas and numpy\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "- Plots\n",
    "```\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "- Scaling\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n",
    "- Split\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "- Pipeline\n",
    "```\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "```\n",
    "- Linear Regression\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "```\n",
    "- Logistic Regression\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```\n",
    "- Decision Trees\n",
    "- RandomForestRegressor\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "```\n",
    "- Random Forest Classifier\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "- XGBRegressor\n",
    "```\n",
    "from xgboost import XGBRegressor\n",
    "```\n",
    "- PCA\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "```\n",
    "- Clustering\n",
    "- Cross Validation\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "```\n",
    "- Evaluation Metrics\n",
    "    - Regression\n",
    "    ```\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    ```\n",
    "    - Classification\n",
    "    ```\n",
    "    from sklearn.metrics import classification_report\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16ec7f",
   "metadata": {},
   "source": [
    "### ML Algos\n",
    "\n",
    "#### 1. Regression\n",
    "- RandomForestRegressor\n",
    "```\n",
    "RandomForestRegressor(random_state=42, n_jobs=-1, max_depth=5, min_samples_leaf=10,\n",
    "                      n_estimators=100)\n",
    "```\n",
    "- XGBRegressor\n",
    "```\n",
    "my_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=-1) \n",
    "my_model_2.fit(X_train, y_train) # Your code here\n",
    "predictions_2 = my_model_2.predict(X_valid)\n",
    "```\n",
    "#### 2. Classification\n",
    "#### 3. Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b245f5",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "- makes preprocessing and modelling steps eaiser for train and test data\n",
    "- saves from human error\n",
    "\n",
    "```\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "preds_test = my_pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b4a54",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "- K Fold Cross Validation with cross_val_score\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "print(\"MAE scores:\\n\", scores)\n",
    "```\n",
    "\n",
    "- GridSearchCV for hyperparameter tuning\n",
    "```\n",
    "rf_2 = RandomForestClassifier(random_state=42, n_jobs=2)\n",
    "params = {\n",
    "    'max_depth': [10,20,40, 50],\n",
    "    'min_samples_leaf': [15,20,50,100,150],\n",
    "    'n_estimators': [100, 125, 150]\n",
    "}\n",
    "folds = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 4)\n",
    "grid_search_2 = GridSearchCV(estimator=rf_2,\n",
    "                           param_grid=params,\n",
    "                           cv = folds,\n",
    "                           n_jobs=2, verbose=1, scoring=\"roc_auc\")\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "rf_best_interpret = grid_search_2.best_estimator_\n",
    "pred_probs_test = rf_best_interpret.predict_proba(X_test)\n",
    "print(classification_report(y_test, rf_best_interpret.predict(X_test)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb8f0e",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "- ANN\n",
    "    - Regression\n",
    "    - Classification\n",
    "- CNN\n",
    "    - Read a digital Image\n",
    "    ```\n",
    "    from keras.datasets import mnist\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "    print(\"The shape of x_train dataset is\", x_train.shape\n",
    "    \n",
    "    # selecting the first sample\n",
    "    x = x_train[1]\n",
    "    print(\"The dimension of x is 2D matrix as \", x.shape)\n",
    "    # Resizing the image\n",
    "    x = cv2.resize(x, (18,18))\n",
    "    \n",
    "    plt.imshow(x, cmap='gray')\n",
    "    \n",
    "    # Reading color image\n",
    "    cat = cv2.imread('cat.jpg')\n",
    "    plt.imshow(cv2.cvtColor(cat, cv2.COLOR_BGR2RGB))\n",
    "    ```\n",
    "    - CIFAR Dataset (Batch Normalization + DropOut)\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.datasets import cifar10\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "    \n",
    "    # Build Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "        \n",
    "    # summary of the model\n",
    "    print(model.summary())\n",
    "\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t              optimizer='sgd',\n",
    "\t              metrics=['accuracy'])\n",
    "\t\n",
    "\tx_train = x_train.astype('float32')\n",
    "\tx_test = x_test.astype('float32')\n",
    "\t\n",
    "\t# Normalizing the input image\n",
    "\tx_train /= 255\n",
    "\tx_test /= 255\n",
    "    \n",
    "    # Training the model\n",
    "\tmodel.fit(x_train, y_train,\n",
    "\t              batch_size=batch_size,\n",
    "\t              epochs=epochs,\n",
    "\t              validation_data=(x_test, y_test),\n",
    "\t              shuffle=True)\n",
    "                  \n",
    "    ```\n",
    "    - Transfer Learning\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\tfrom tensorflow import keras\n",
    "\tfrom tensorflow.keras import layers, optimizers\n",
    "\tfrom tensorflow.keras.layers import Input, Add,Dropout, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "\tfrom tensorflow.keras.models import Model, load_model\n",
    "\tfrom tensorflow.keras.preprocessing import image\n",
    "\tfrom tensorflow.keras.utils import  plot_model\n",
    "\tfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\tfrom tensorflow.keras.initializers import glorot_uniform\n",
    "\tfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "\tfrom tensorflow.keras.applications import ResNet50\n",
    "\tfrom keras.applications.resnet import preprocess_input\n",
    "    ```\n",
    "        - Method 1 (Use pretrained model as is, just change last few layers)\n",
    "\t    ```\n",
    "\t     base_model = ResNet50(weights='imagenet', include_top=False)    \n",
    "\t     # As we are using ResNet model only for feature extraction and not adjusting the weights\n",
    "\t     # we freeze the layers in base model\n",
    "\t     for layer in base_model.layers:\n",
    "\t         layer.trainable = False\n",
    "\t        \n",
    "\t     # Get base model output \n",
    "\t     base_model_ouput = base_model.output\n",
    "\t    \n",
    "\t     # Adding our own layer \n",
    "\t     x = GlobalAveragePooling2D()(base_model_ouput)\n",
    "\t     # Adding fully connected layer\n",
    "\t     x = Dense(512, activation='relu')(x)\n",
    "\t     x = Dense(num_classes, activation='softmax', name='fcnew')(x)\n",
    "\t    \n",
    "\t     model = Model(inputs=base_model.input, outputs=x)\n",
    "\t     model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\t    \n",
    "\t     image_size = 224\n",
    "\t\t batch_size = 64\n",
    "\n",
    "\t\t train_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "\t\t    shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\t\t valid_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\t\t train_generator = train_data_gen.flow_from_directory(train_dir, (image_size,image_size), batch_size=batch_size, class_mode='categorical')\n",
    "\t\t valid_generator = valid_data_gen.flow_from_directory(test_dir, (image_size,image_size), batch_size=batch_size, class_mode='categorical')\n",
    "         \n",
    "\t\t model.fit(\n",
    "\t\t     train_generator,\n",
    "\t\t     steps_per_epoch=train_generator.n//batch_size,\n",
    "\t\t     validation_data=valid_generator,\n",
    "\t\t     validation_steps=valid_generator.n//batch_size,\n",
    "\t\t     epochs=epochs)         \n",
    "\t    ```\n",
    "        - Method 2(Freeze top 140 layers, retrain rest (better performance))\n",
    "        ```\n",
    "\t\t epochs = 10\n",
    "\t\t split_at = 140\n",
    "         \n",
    "         #freeze top 140 layers and train after that\n",
    "         for layer in model.layers[:split_at]: \n",
    "\t\t   layer.trainable = False\n",
    "\t\t for layer in model.layers[split_at:]: \n",
    "\t\t   layer.trainable = True          \t\t    \n",
    "\t\t model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "         \n",
    "\t\t # Choosing lower learning rate for fine-tuning\n",
    "\t\t # learning rate is generally 10-1000 times lower than normal learning rate, if we are fine tuning the initial layers\n",
    "         \n",
    "\t\t sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\t\t model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "         \n",
    "\t\t model.fit_generator(\n",
    "\t\t    train_generator,\n",
    "\t\t    steps_per_epoch=train_generator.n//batch_size,\n",
    "\t\t    validation_data=valid_generator,\n",
    "\t\t    validation_steps=valid_generator.n//batch_size,\n",
    "\t\t    epochs=epochs,\n",
    "\t\t    verbose=1)         \n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10232f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
